## 목차
* [프로젝트 개요](#프로젝트-개요)
* [문제 인식](#문제-인식)
* [해결 방안](#해결-방안)
* [사용 기술](#사용-기술)
* [실행 방법](#실행-방법)
* [시사점](#시사점)


## 프로젝트 개요
데이터베이스 설계 및 최적화 - Restful API의 Latency 단축, 서버 과부하 문제 해결
- 350만 row 데이터의 재설계를 통한 API Latency 85%, 서버 메모리 사용량 85% 감축
- 서비스 형태에 최적화된 데이터베이스 및 API 재설계

## 문제 인식
1. 데이터 통계 (평균, 순위 등) 결과를 제공하는 API의 Latency가 최대 5초였으며, API 요청 시마다 높은 RAM 사용량(70% 이상)으로 서버 과부하 문제가 발생
2. 비용이 증가하는 Vertical Scaling 방식은 최대한 지양하고, 아이디어와 기술력을 통해 문제를 해결하고자 하였음
3. row 수가 많은 만큼 통계 결과가 매시간마다 크게 변하지는 않는다는 점, 사용자가 선택할 수 있는 조합은 유한하다는 점에 착안
4. 서버 과부하의 원인은 트래픽 과다 혹은 API 코드의 문제가 아니라, API 요청 시 메모리에 로드되는 row가 많은 데 있음에 착안
5. 수백만 검색 결과를 0.5초 이내로 제시하는 구글을 벤치마킹하여, 최대 1초 이내로 Latency를 80% 단축하겠다는 목표를 설정

## 해결 방안
1. 사용자가 선택할 수 있는 모든 조합(1000가지)에 대한 통계 결과를 모두 미리 계산하여 별도 table에 삽입
2. 사용자가 가장 적게 몰리는 새벽 시간대에 하루 1번, 최신 데이터를 포함해 통계 결과를 재계산하도록 Cron Job을 설정하고 데이터 파이프라인을 구축
3. 각 API가 접근하는 row 수를 1개로 하여 RAM 부하를 줄였으며, 접근 시 사용하는 key 3개를 모두 인덱싱 처리하여 데이터 접근 속도를 향상

## 사용 기술
* Python 3.8 (Flask 2.2)
* MongoDB  
* gunicorn
	
## 실행 방법
1. `git clone git@github.com:z3zzz/newsgamjeong-datacloud.git` 
2. 환경변수 설정 (MONGODB_URL)
3. 각 데이터 유형 별로 파일 실행
- 코로나19 데이터: infectionos.py
- 뉴스 데이터: prepare1.py -> prepare2.py -> prepare2_new.py -> prepare3.py -> prepare3_new.py -> insert.py
- 워드클라우드, lda 데이터: wordcloud.py -> wordcloud2.py, lda.py

## 차별점
1. 2020-2021년도 코로나 및 뉴스데이터 350만 row의 통계 시각화 제공이라는 서비스 목표에 최적화된 데이터 스키마 및 API 설계
2. Job, Shell Script 설정을 통한 파일 실행 자동화


## 시사점
1. Latency가 최대 5초 -> 0.7초 이내로 85%, API 요청시의 RAM 부하율이 70% -> 10% 이내로 85% 감소
2. 데이터의 양에 집중하기보다도, 어떤 유형의 서비스이고 어느 특성의 데이터가 제공되어야 하는지를 분석하는 것이 문제 접근 방법이라는 점을 학습
